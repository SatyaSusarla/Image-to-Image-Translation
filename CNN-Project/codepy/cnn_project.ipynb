{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras import models, layers"
      ],
      "metadata": {
        "id": "VAauSNPaOKi0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "\n",
        "def build_cnn(input_shape=(256, 256, 3)):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Encoder\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Bottleneck\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    # Decoder\n",
        "    model.add(UpSampling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    model.add(UpSampling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))  # Output layer\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build and compile the CNN model\n",
        "cnn_model = build_cnn()\n",
        "cnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ZEImCYnA3H2Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_input(directory, target_size=(256, 256)):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.jpg'):\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Could not read image {filename}\")\n",
        "                continue\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, target_size)\n",
        "            # Simulate conversion to PNG (just for the sake of using in CNN, not saving)\n",
        "            # You can directly use 'img' here in your CNN training\n",
        "            images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "def load_images_output(directory, target_size=(256, 256)):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.png'):  # Change to check for PNG files\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Could not read image {filename}\")\n",
        "                continue\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, target_size)\n",
        "            images.append(img)\n",
        "    return np.array(images)"
      ],
      "metadata": {
        "id": "Qa--OwXm3LgH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "io_dir = '/content/drive/MyDrive/Data-Photos'  # Input directory containing JPG images\n",
        "op_dir = '/content/drive/MyDrive/Final-Photos'  # Output directory for processed PNG images\n",
        "\n",
        "input_images = load_images_input(io_dir)\n",
        "output_images = load_images_output(op_dir)\n"
      ],
      "metadata": {
        "id": "KcoimMk_3Oa_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.fit(input_images / 255.0, output_images / 255.0, epochs=50, batch_size=16, validation_split=0.1)\n",
        "cnn_model.save('cnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0h5p6MRFwLE",
        "outputId": "f5aba0f0-474b-4be0-cf80-e82c3793f9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "30/30 [==============================] - 1001s 33s/step - loss: 0.0721 - accuracy: 0.6191 - val_loss: 0.0800 - val_accuracy: 0.4941\n",
            "Epoch 2/50\n",
            "30/30 [==============================] - 1005s 34s/step - loss: 0.0709 - accuracy: 0.6305 - val_loss: 0.0761 - val_accuracy: 0.4941\n",
            "Epoch 3/50\n",
            "30/30 [==============================] - 1002s 33s/step - loss: 0.0705 - accuracy: 0.6305 - val_loss: 0.0833 - val_accuracy: 0.4941\n",
            "Epoch 4/50\n",
            "30/30 [==============================] - 979s 33s/step - loss: 0.0716 - accuracy: 0.6294 - val_loss: 0.0753 - val_accuracy: 0.4941\n",
            "Epoch 5/50\n",
            "30/30 [==============================] - 971s 32s/step - loss: 0.0709 - accuracy: 0.6306 - val_loss: 0.0759 - val_accuracy: 0.4941\n",
            "Epoch 6/50\n",
            "30/30 [==============================] - 945s 32s/step - loss: 0.0706 - accuracy: 0.6306 - val_loss: 0.0742 - val_accuracy: 0.4941\n",
            "Epoch 7/50\n",
            "30/30 [==============================] - 982s 33s/step - loss: 0.0706 - accuracy: 0.6295 - val_loss: 0.0792 - val_accuracy: 0.4941\n",
            "Epoch 8/50\n",
            "30/30 [==============================] - 983s 33s/step - loss: 0.0706 - accuracy: 0.6306 - val_loss: 0.0753 - val_accuracy: 0.4941\n",
            "Epoch 9/50\n",
            "30/30 [==============================] - 979s 33s/step - loss: 0.0706 - accuracy: 0.6306 - val_loss: 0.0742 - val_accuracy: 0.4941\n",
            "Epoch 10/50\n",
            "30/30 [==============================] - 970s 32s/step - loss: 0.0703 - accuracy: 0.6306 - val_loss: 0.0747 - val_accuracy: 0.4941\n",
            "Epoch 11/50\n",
            "30/30 [==============================] - 998s 33s/step - loss: 0.0701 - accuracy: 0.6305 - val_loss: 0.0757 - val_accuracy: 0.4940\n",
            "Epoch 12/50\n",
            "30/30 [==============================] - 1039s 35s/step - loss: 0.0708 - accuracy: 0.6305 - val_loss: 0.0723 - val_accuracy: 0.4938\n",
            "Epoch 13/50\n",
            "30/30 [==============================] - 1057s 35s/step - loss: 0.0707 - accuracy: 0.6306 - val_loss: 0.0737 - val_accuracy: 0.4939\n",
            "Epoch 14/50\n",
            "30/30 [==============================] - 1046s 35s/step - loss: 0.0704 - accuracy: 0.6307 - val_loss: 0.0726 - val_accuracy: 0.4937\n",
            "Epoch 15/50\n",
            "30/30 [==============================] - 1044s 35s/step - loss: 0.0708 - accuracy: 0.6306 - val_loss: 0.0761 - val_accuracy: 0.4941\n",
            "Epoch 16/50\n",
            "30/30 [==============================] - 1042s 35s/step - loss: 0.0706 - accuracy: 0.6306 - val_loss: 0.0769 - val_accuracy: 0.4941\n",
            "Epoch 17/50\n",
            "30/30 [==============================] - 1044s 35s/step - loss: 0.0703 - accuracy: 0.6306 - val_loss: 0.0749 - val_accuracy: 0.4944\n",
            "Epoch 18/50\n",
            "30/30 [==============================] - 1046s 35s/step - loss: 0.0703 - accuracy: 0.6305 - val_loss: 0.0746 - val_accuracy: 0.4935\n",
            "Epoch 19/50\n",
            "30/30 [==============================] - 1046s 35s/step - loss: 0.0702 - accuracy: 0.6306 - val_loss: 0.0728 - val_accuracy: 0.4941\n",
            "Epoch 20/50\n",
            "30/30 [==============================] - 1038s 35s/step - loss: 0.0705 - accuracy: 0.6306 - val_loss: 0.0736 - val_accuracy: 0.4940\n",
            "Epoch 21/50\n",
            "30/30 [==============================] - 1047s 35s/step - loss: 0.0701 - accuracy: 0.6306 - val_loss: 0.0724 - val_accuracy: 0.4936\n",
            "Epoch 22/50\n",
            "30/30 [==============================] - 1048s 35s/step - loss: 0.0702 - accuracy: 0.6306 - val_loss: 0.0755 - val_accuracy: 0.4940\n",
            "Epoch 23/50\n",
            "30/30 [==============================] - 1035s 35s/step - loss: 0.0701 - accuracy: 0.6306 - val_loss: 0.0755 - val_accuracy: 0.4940\n",
            "Epoch 24/50\n",
            "30/30 [==============================] - 1044s 35s/step - loss: 0.0702 - accuracy: 0.6306 - val_loss: 0.0757 - val_accuracy: 0.4940\n",
            "Epoch 25/50\n",
            "30/30 [==============================] - 1041s 35s/step - loss: 0.0700 - accuracy: 0.6306 - val_loss: 0.0737 - val_accuracy: 0.4940\n",
            "Epoch 26/50\n",
            "30/30 [==============================] - 1038s 35s/step - loss: 0.0700 - accuracy: 0.6306 - val_loss: 0.0714 - val_accuracy: 0.4940\n",
            "Epoch 27/50\n",
            "30/30 [==============================] - 1048s 35s/step - loss: 0.0702 - accuracy: 0.6305 - val_loss: 0.0762 - val_accuracy: 0.4941\n",
            "Epoch 28/50\n",
            "30/30 [==============================] - 1036s 35s/step - loss: 0.0706 - accuracy: 0.6306 - val_loss: 0.0771 - val_accuracy: 0.4941\n",
            "Epoch 29/50\n",
            "30/30 [==============================] - 1035s 35s/step - loss: 0.0703 - accuracy: 0.6304 - val_loss: 0.0755 - val_accuracy: 0.4930\n",
            "Epoch 30/50\n",
            "30/30 [==============================] - 1044s 35s/step - loss: 0.0700 - accuracy: 0.6305 - val_loss: 0.0785 - val_accuracy: 0.4941\n",
            "Epoch 31/50\n",
            "30/30 [==============================] - 1044s 35s/step - loss: 0.0702 - accuracy: 0.6306 - val_loss: 0.0740 - val_accuracy: 0.4940\n",
            "Epoch 32/50\n",
            "30/30 [==============================] - 1044s 35s/step - loss: 0.0701 - accuracy: 0.6306 - val_loss: 0.0755 - val_accuracy: 0.4941\n",
            "Epoch 33/50\n",
            "30/30 [==============================] - 1053s 35s/step - loss: 0.0699 - accuracy: 0.6306 - val_loss: 0.0762 - val_accuracy: 0.4941\n",
            "Epoch 34/50\n",
            "30/30 [==============================] - 1050s 35s/step - loss: 0.0699 - accuracy: 0.6305 - val_loss: 0.0758 - val_accuracy: 0.4941\n",
            "Epoch 35/50\n",
            "30/30 [==============================] - 1042s 35s/step - loss: 0.0697 - accuracy: 0.6305 - val_loss: 0.0701 - val_accuracy: 0.4940\n",
            "Epoch 36/50\n",
            "30/30 [==============================] - 1039s 35s/step - loss: 0.0699 - accuracy: 0.6305 - val_loss: 0.0733 - val_accuracy: 0.4941\n",
            "Epoch 37/50\n",
            "30/30 [==============================] - 1041s 35s/step - loss: 0.0702 - accuracy: 0.6306 - val_loss: 0.0723 - val_accuracy: 0.4941\n",
            "Epoch 38/50\n",
            "30/30 [==============================] - 1033s 35s/step - loss: 0.0698 - accuracy: 0.6305 - val_loss: 0.0718 - val_accuracy: 0.4940\n",
            "Epoch 39/50\n",
            "30/30 [==============================] - 1036s 35s/step - loss: 0.0700 - accuracy: 0.6306 - val_loss: 0.0737 - val_accuracy: 0.4941\n",
            "Epoch 40/50\n",
            "30/30 [==============================] - 1038s 35s/step - loss: 0.0699 - accuracy: 0.6306 - val_loss: 0.0728 - val_accuracy: 0.4941\n",
            "Epoch 41/50\n",
            "22/30 [=====================>........] - ETA: 4:27 - loss: 0.0679 - accuracy: 0.6241"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "reloaded_model = tf.keras.models.load_model('cnn_model.h5')\n",
        "\n",
        "# Load new images\n",
        "new_input_images =  load_image(input_image_path)\n",
        "new_output_predictions = reloaded_model.predict(new_input_images / 255.0)\n",
        "\n",
        "# Save the new predictions\n",
        "save_images(new_output_predictions, 'path/to/save_new_converted_images', os.listdir(io_dir))\n"
      ],
      "metadata": {
        "id": "QKOL2nts3aZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "directory_path = '/content/drive/MyDrive/Data-Photos'\n",
        "\n",
        "# Count the number of JPG files\n",
        "jpg_count = sum(1 for filename in os.listdir(directory_path) if filename.lower().endswith('.png'))\n",
        "\n",
        "print(f\"Number of JPG files: {jpg_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOsxjL7a_Hxi",
        "outputId": "9901e01e-4f1a-41ee-85ea-ff38a861b72e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of JPG files: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}